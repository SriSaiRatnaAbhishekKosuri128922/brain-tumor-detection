{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SriSaiRatnaAbhishekKosuri128922/brain-tumor-detection/blob/main/Brain_Tumor_Detection_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Brain Tumor Detection Based on Deep Learning Approaches and MRI\n",
        "\n",
        "---\n",
        "\n",
        "### Project By:  \n",
        "**Sri Sai Ratna Abhishek Kosuri**  \n",
        "B.Tech CSE with AI & ML  \n",
        "VIT-AP University\n",
        "\n",
        "---\n",
        "\n",
        "This project was independently developed as part of my final year academic work.\n"
      ],
      "metadata": {
        "id": "bJdB49el_yiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßæ Introduction\n",
        "\n",
        "Brain tumors are a severe health concern and early diagnosis is crucial for effective treatment. Manual analysis of MRI scans can be time-consuming and prone to errors, which is where deep learning models can assist significantly.\n",
        "\n",
        "This project aims to build a deep learning model using PyTorch to classify brain tumor MRI images into appropriate categories. The approach involves dataset preprocessing, augmentation using Albumentations, model architecture design, and evaluation on performance metrics such as accuracy and ROC-AUC.\n",
        "\n",
        "The dataset is downloaded using the Kaggle API and contains labeled MRI images.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "hF9hgT69Ai6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Objective\n",
        "\n",
        "The main objective of this project is to:\n",
        "\n",
        "- Develop a deep learning model to classify brain tumor MRI scans.\n",
        "- Apply image preprocessing and augmentation techniques to enhance data quality.\n",
        "- Train and evaluate the model using performance metrics such as accuracy, confusion matrix, and ROC-AUC score.\n",
        "- Demonstrate the feasibility of AI-assisted diagnosis in medical imaging.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "LsCn_dgTCmhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Tools & Technologies\n",
        "\n",
        "The project utilizes the following tools, libraries, and platforms:\n",
        "\n",
        "- **Programming Language:** Python  \n",
        "- **Frameworks & Libraries:**  \n",
        "  - PyTorch, Torchvision, Albumentations, TorchSummary, TorchViz  \n",
        "  - TensorFlow (for comparison/utility purposes)  \n",
        "  - OpenCV, PIL (image processing)  \n",
        "  - Pandas, NumPy (data manipulation)  \n",
        "  - Matplotlib, Seaborn (visualization)  \n",
        "- **Machine Learning Utilities:**  \n",
        "  - Scikit-learn (model evaluation, data splitting)  \n",
        "- **Platform:** Google Colab  \n",
        "- **Data Source:** [Kaggle - Brain Tumor MRI Scans](https://www.kaggle.com/datasets/rm1000/brain-tumor-mri-scans)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "4PsxnhfgCsCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Required Libraries"
      ],
      "metadata": {
        "id": "ZW544Dzz-p49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Libraries\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Image Processing\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Data Manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from termcolor import colored\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Random, Zip File, and TensorFlow Utilities\n",
        "import random\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "\n",
        "# Deep Learning Libraries (PyTorch)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.optim import lr_scheduler\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Utility Libraries\n",
        "import copy\n",
        "import time\n",
        "\n",
        "# Install additional packages\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot\n"
      ],
      "metadata": {
        "id": "9G48qX5RY3ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Device Selection"
      ],
      "metadata": {
        "id": "Jr__zqg6DOm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n"
      ],
      "metadata": {
        "id": "t3qR2mbNZWLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle API Setup"
      ],
      "metadata": {
        "id": "Gtvrjn-CDT-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîë Setting up Kaggle API to access the dataset\n",
        "# Create the Kaggle directory\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "# Move kaggle.json to the correct folder\n",
        "os.system(\"cp /content/kaggle.json /root/.kaggle/\")\n",
        "# Set permissions to avoid API errors\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n"
      ],
      "metadata": {
        "id": "eAB_tFFOZWHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list -s \"brain tumor MRI\"\n"
      ],
      "metadata": {
        "id": "eXmn92HEZWF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset using Kaggle API\n",
        "!kaggle datasets download -d rm1000/brain-tumor-mri-scans\n"
      ],
      "metadata": {
        "id": "psWdxGOJZWC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Explore the Dataset"
      ],
      "metadata": {
        "id": "oFnLI1aNHrLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üì¶ Unzip Dataset and Define Paths"
      ],
      "metadata": {
        "id": "NHXdcb0UHuBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(\"brain-tumor-mri-scans.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"brain_tumor_dataset\")\n",
        "\n",
        "# Define dataset path\n",
        "dataset_path = \"/content/brain_tumor_dataset\"\n"
      ],
      "metadata": {
        "id": "WvZAK-bkZWAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìÅ Check Dataset Structure"
      ],
      "metadata": {
        "id": "xJiYpET0H5Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset files and structure:\")\n",
        "!ls brain_tumor_dataset\n"
      ],
      "metadata": {
        "id": "53lKxS3hH2Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîç View Class Names"
      ],
      "metadata": {
        "id": "98v3EtxgH-hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = os.listdir(dataset_path)\n",
        "class_names.sort()\n",
        "print(class_names)\n"
      ],
      "metadata": {
        "id": "XOkhZHXaTTYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä Count Images in Each Class"
      ],
      "metadata": {
        "id": "s15ezxrdIDXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize class count dictionary\n",
        "categories = {\"glioma\": 0, \"healthy\": 0, \"meningioma\": 0, \"pituitary\": 0}\n",
        "\n",
        "# Count images in each class\n",
        "for category in categories:\n",
        "    category_path = os.path.join(dataset_path, category)\n",
        "    categories[category] = len(os.listdir(category_path))\n",
        "    print(f\"üìÇ {category}: {categories[category]} images\")\n",
        "\n",
        "print(f\"\\nüì¶ Total Images: {sum(categories.values())}\")\n"
      ],
      "metadata": {
        "id": "Vu5BMFXsZV9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìà Visualize Class Distribution"
      ],
      "metadata": {
        "id": "L1QS86eiIN97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for plotting\n",
        "df = pd.DataFrame({\n",
        "    'Class': list(categories.keys()),\n",
        "    'Count': list(categories.values())\n",
        "})\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Barplot\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(y='Class', x='Count', data=df, palette='muted')\n",
        "plt.title('Distribution of Classes')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Class Name')\n",
        "ax = plt.gca()\n",
        "for p in ax.patches:\n",
        "    width = p.get_width()\n",
        "    ax.annotate(f'{int(width)}', (width + 5, p.get_y() + p.get_height() / 2),\n",
        "                ha='left', va='center')\n",
        "sns.despine(left=True, bottom=True)\n",
        "\n",
        "# Pie Chart\n",
        "plt.subplot(1, 2, 2)\n",
        "df.set_index('Class')['Count'].plot.pie(autopct='%1.1f%%',\n",
        "                                        colors=sns.color_palette('muted'),\n",
        "                                        startangle=90,\n",
        "                                        explode=[0.05]*len(df))\n",
        "plt.title('Percentage Distribution of Classes')\n",
        "plt.ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ehbvOJNjFH5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üñºÔ∏è Preview a Sample Image"
      ],
      "metadata": {
        "id": "VYXqDshtInmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = \"/content/brain_tumor_dataset/glioma/0812.jpg\"\n",
        "img = tf.io.read_file(img_path)\n",
        "img = tf.image.decode_jpeg(img, channels=3)\n",
        "print(\"Sample image shape:\", img.shape)\n"
      ],
      "metadata": {
        "id": "NBPSXYiuJrXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîç List Extracted Files"
      ],
      "metadata": {
        "id": "OKdlOKvFI0x-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all image paths for review/debug\n",
        "for dirname, _, filenames in os.walk(dataset_path):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "1o3b3LDlZV7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Transformations and DataLoader Preparation"
      ],
      "metadata": {
        "id": "mXFG9oQXKMsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öôÔ∏è Hyperparameters"
      ],
      "metadata": {
        "id": "y9UpAR_oI9dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define global parameters\n",
        "batch_size = 8\n",
        "img_size = 224\n",
        "num_epochs = 45\n"
      ],
      "metadata": {
        "id": "oeSTBIncC8X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation using Albumentations"
      ],
      "metadata": {
        "id": "BLRXkTcPEQqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.HorizontalFlip(),\n",
        "        A.RandomBrightnessContrast(),\n",
        "        A.Rotate(limit=15),\n",
        "        A.CoarseDropout(max_holes=1, max_height=16, max_width=16, fill_value=0),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ]),\n",
        "    'val': A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "}\n"
      ],
      "metadata": {
        "id": "9auEcfW5ZV4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Dataset Loader"
      ],
      "metadata": {
        "id": "UKTJiWnGEWB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(datasets.ImageFolder):\n",
        "    def __init__(self, root, transform=None):\n",
        "        super(CustomImageDataset, self).__init__(root, transform=None)\n",
        "        self.custom_transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, label = super(CustomImageDataset, self).__getitem__(index)\n",
        "        image = np.array(image)\n",
        "        if self.custom_transform:\n",
        "            augmented = self.custom_transform(image=image)\n",
        "            image = augmented['image']\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "ZeBB__krZV2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Loading and Splitting"
      ],
      "metadata": {
        "id": "rWdlAuWVEZuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = CustomImageDataset(dataset_path, transform=data_transforms['train'])\n",
        "\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size = int(0.15 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "val_dataset.dataset.custom_transform = data_transforms['val']\n",
        "test_dataset.dataset.custom_transform = data_transforms['val']\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Print dataset information for the report\n",
        "print(\"Dataset Summary:\\n\")\n",
        "print(f\"Total Images: {len(full_dataset)}\\n\")\n",
        "print(f\"Training Set Size: {len(train_dataset)} (70%)\\n\")\n",
        "print(f\"Validation Set Size: {len(val_dataset)} (15%)\\n\")\n",
        "print(f\"Test Set Size: {len(test_dataset)} (15%)\\n\")\n",
        "\n",
        "print(\"\\nDataLoader Details:\\n\")\n",
        "print(f\"Train DataLoader: batch_size={batch_size}, shuffle=True\\n\")\n",
        "print(f\"Validation DataLoader: batch_size={batch_size}, shuffle=False\\n\")\n",
        "print(f\"Test DataLoader: batch_size={batch_size}, shuffle=False\\n\")"
      ],
      "metadata": {
        "id": "LK15_eHVZVzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Pie Chart Overview"
      ],
      "metadata": {
        "id": "-Z8nZa4lKX-R"
      }
    },
    {
      "source": [
        "# Data for the pie chart\n",
        "labels = ['Training Set', 'Validation Set', 'Testing Set']\n",
        "sizes = [len(train_dataset), len(val_dataset), len(test_dataset)]\n",
        "colors = ['#FF5733', '#FFC300', '#36A2EB']\n",
        "\n",
        "# Create the pie chart\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Dataset Distribution')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aofb5TcNVw9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Distribution in Train, Val, Test"
      ],
      "metadata": {
        "id": "ela24g41Kbr-"
      }
    },
    {
      "source": [
        "def get_class_distribution(dataset):\n",
        "    class_counts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
        "    for _, label in dataset:\n",
        "        class_counts[label] += 1\n",
        "    return list(class_counts.values())\n",
        "\n",
        "# Get class distributions for each set\n",
        "train_dist = get_class_distribution(train_dataset)\n",
        "val_dist = get_class_distribution(val_dataset)\n",
        "test_dist = get_class_distribution(test_dataset)\n",
        "\n",
        "# Create pie charts\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Training set pie chart\n",
        "axs[0].pie(train_dist, labels=class_names, autopct='%1.1f%%', startangle=90)\n",
        "axs[0].set_title('Training Set Class Distribution')\n",
        "\n",
        "# Validation set pie chart\n",
        "axs[1].pie(val_dist, labels=class_names, autopct='%1.1f%%', startangle=90)\n",
        "axs[1].set_title('Validation Set Class Distribution')\n",
        "\n",
        "# Testing set pie chart\n",
        "axs[2].pie(test_dist, labels=class_names, autopct='%1.1f%%', startangle=90)\n",
        "axs[2].set_title('Testing Set Class Distribution')\n",
        "\n",
        "for ax in axs:\n",
        "    ax.axis('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cVBNJtSLWDSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Size Statistics"
      ],
      "metadata": {
        "id": "r0w1Dd6QKmhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_sizes = []  # Collect image sizes\n",
        "\n",
        "for dirname, _, filenames in os.walk(dataset_path):\n",
        "    for filename in filenames:\n",
        "        img_path = os.path.join(dirname, filename)\n",
        "        img = Image.open(img_path)\n",
        "        image_sizes.append(img.size)\n",
        "\n",
        "# Analyze and print image size statistics\n",
        "image_sizes_df = pd.DataFrame(image_sizes, columns=['Width', 'Height'])\n",
        "print(\"\\nImage Size Statistics:\")\n",
        "print(image_sizes_df.describe())"
      ],
      "metadata": {
        "id": "j6vl2l1bg-OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Matrix, PCA, and t-SNE"
      ],
      "metadata": {
        "id": "hezD7bPcKtvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Matrix and Heatmap"
      ],
      "metadata": {
        "id": "d0sLGodhLStj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated features for illustration\n",
        "image_features = np.random.rand(100, 5)\n",
        "\n",
        "# Calculate Correlation Matrix\n",
        "correlation_matrix = np.corrcoef(image_features, rowvar=False)\n",
        "\n",
        "# Visualize using heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix of Image Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Nh1FMGphNpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA)"
      ],
      "metadata": {
        "id": "nP0_mngTLctS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform PCA\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(image_features)\n",
        "\n",
        "# Visualize principal components\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(principal_components[:, 0], principal_components[:, 1])\n",
        "plt.title('PCA Visualization')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lydVaGbOhlxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-distributed Stochastic Neighbor Embedding (t-SNE)"
      ],
      "metadata": {
        "id": "DSFMFSLjLjwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2)\n",
        "tsne_results = tsne.fit_transform(image_features)\n",
        "\n",
        "# Visualize t-SNE results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(tsne_results[:, 0], tsne_results[:, 1])\n",
        "plt.title('t-SNE Visualization')\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JZrD76abhq2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Samples Visualization"
      ],
      "metadata": {
        "id": "G7S1ZkMcEjf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying Raw Image Samples per Class"
      ],
      "metadata": {
        "id": "Qsp7UDfINQtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Purpose: Displays num_samples original images from each class folder to visually inspect the dataset content."
      ],
      "metadata": {
        "id": "zn3anEyfNZrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_class_samples(dataset_path, classes, num_samples=3):\n",
        "    fig, axes = plt.subplots(len(classes), num_samples, figsize=(num_samples * 3, len(classes) * 3))\n",
        "    for i, cls in enumerate(classes):\n",
        "        class_dir = os.path.join(dataset_path, cls)\n",
        "        images = random.sample(os.listdir(class_dir), min(num_samples, len(os.listdir(class_dir))))\n",
        "        for j, img_name in enumerate(images):\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            img = Image.open(img_path)\n",
        "            axes[i, j].imshow(img)\n",
        "            axes[i, j].axis('off')\n",
        "            if j == 0:\n",
        "                 axes[i, j].set_title(f\"{cls} samples\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "show_class_samples(dataset_path, class_names)"
      ],
      "metadata": {
        "id": "1gwMd-78ZVxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying Augmented Samples Using Albumentations"
      ],
      "metadata": {
        "id": "qfgkkkpyNgu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Purpose: Shows transformed images after applying data augmentation pipeline. Ensures that transformations like rotation, shift, flip, etc., are working as intended."
      ],
      "metadata": {
        "id": "e2q5lpitN2xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_augmented_samples(dataset_path, classes, num_samples=3):\n",
        "    fig, axes = plt.subplots(len(classes), num_samples, figsize=(num_samples * 3, len(classes) * 3))\n",
        "\n",
        "    for i, cls in enumerate(classes):\n",
        "        class_dir = os.path.join(dataset_path, cls)  # Get directory for current class\n",
        "        image_files = [\n",
        "            os.path.join(class_dir, filename)\n",
        "            for filename in os.listdir(class_dir)\n",
        "            if filename.endswith(('.jpg', '.jpeg', '.png'))\n",
        "        ]\n",
        "\n",
        "        selected_images = random.sample(image_files, num_samples)\n",
        "\n",
        "        for j, img_path in enumerate(selected_images):\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
        "\n",
        "            augmented_img = data_transforms['train'](image=img)['image']\n",
        "\n",
        "            # Display augmented image\n",
        "            axes[i, j].imshow(augmented_img.permute(1, 2, 0))\n",
        "            axes[i, j].axis('off')\n",
        "            if j == 0:\n",
        "                axes[i, j].set_title(f\"{cls} samples\", fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Call the function\n",
        "show_augmented_samples(dataset_path, class_names)"
      ],
      "metadata": {
        "id": "0kZ_OOPIh013"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Image Batch Shape from DataLoader"
      ],
      "metadata": {
        "id": "JSTzMn-6N7Ar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Purpose: Confirms the shape of input image tensors loaded by the train_loader. Useful for debugging batch size, channel format, or preprocessing mismatches."
      ],
      "metadata": {
        "id": "oRzieupON_8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_loader:\n",
        "    print(\"Batch image shape:\", images.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "jVJUd_ARMCk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original vs Transformed Image"
      ],
      "metadata": {
        "id": "dEj9kUwgOOVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the image using TensorFlow\n",
        "img_path = \"/content/brain_tumor_dataset/glioma/0812.jpg\"\n",
        "img_tf = tf.io.read_file(img_path)\n",
        "img_tf = tf.image.decode_jpeg(img_tf, channels=3)\n",
        "print(\"TensorFlow Image shape:\", img_tf.shape)\n",
        "\n",
        "# 2. Define the transformations used by PyTorch DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 3. Convert the TensorFlow tensor to a PIL image\n",
        "img_pil = Image.fromarray(img_tf.numpy())\n",
        "\n",
        "# 4. Apply the PyTorch transforms\n",
        "img_torch = transform(img_pil)\n",
        "print(\"PyTorch Image shape:\", img_torch.shape)\n",
        "\n",
        "# 5. Display the original and transformed images\n",
        "\n",
        "#  Original image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img_tf.numpy())\n",
        "plt.title(\"Original Img\")\n",
        "plt.axis('off')\n",
        "\n",
        "#  Transformed image\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(img_torch.permute(1, 2, 0).numpy())\n",
        "plt.title(\"Transformed Img\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CtkVEiRQN_yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture (ResNet50)"
      ],
      "metadata": {
        "id": "xi3cOzBiEqaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resnet50\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 4)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "9wZRLybIZVuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function, Optimizer, and Scheduler"
      ],
      "metadata": {
        "id": "VchCs6HJEwqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)"
      ],
      "metadata": {
        "id": "7bwwuBQYZVsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation Functions"
      ],
      "metadata": {
        "id": "CXWvGzCiE8Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Function with Early Stopping and Scheduler"
      ],
      "metadata": {
        "id": "Vhf9UlEkPrll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Purpose: Trains the model while tracking metrics, implementing early stopping, and using a scheduler to adapt learning rate."
      ],
      "metadata": {
        "id": "723IJiNlPgPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=5):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    # Early stopping variables\n",
        "    early_stopping_counter = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        # --- Training phase ---\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        # --- Validation phase ---\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = 100 * correct / total\n",
        "\n",
        "        print(f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Record history\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        # --- Early stopping ---\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "            if early_stopping_counter >= patience:\n",
        "                print(f\"Early stopping triggered after {patience} epochs without improvement!\")\n",
        "                break\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n"
      ],
      "metadata": {
        "id": "ndI39_dEaEZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation Function"
      ],
      "metadata": {
        "id": "QjLlYXq0Pmat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Purpose: Evaluates model performance on any dataset (train/val/test), returning loss and accuracy."
      ],
      "metadata": {
        "id": "9Dpa9sp3P5xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return running_loss / len(data_loader), 100 * correct / total\n"
      ],
      "metadata": {
        "id": "Eu-i7k7OPnuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Summary (Using torchsummary)"
      ],
      "metadata": {
        "id": "0e4VDSbmP88Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Purpose: Prints the model architecture layer-by-layer with output shapes and parameter counts (like model.summary() in Keras)."
      ],
      "metadata": {
        "id": "M3hWx1FZP_cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model summary\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "--Zt2pjhW-BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print Layer Names in Model"
      ],
      "metadata": {
        "id": "EplGj-_UQDGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Purpose: Prints all named modules (layers) in the model. Useful for inspection or modification of specific layers."
      ],
      "metadata": {
        "id": "YjWOabBmQKLt"
      }
    },
    {
      "source": [
        "for i, (name, _) in enumerate(model.named_modules()):\n",
        "    print(f\"layer{i}: {name}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EbkCFVKuXXFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Model Architecture as a Graph (Using torchviz)"
      ],
      "metadata": {
        "id": "jWRUbUz6QZqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Purpose: Generates a graph-based visualization of the model's computation graph and saves it as an image."
      ],
      "metadata": {
        "id": "qdGEJ97NQg10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dummy input tensor\n",
        "dummy_input = torch.randn(1, 3, 224, 224, device=device)  # Input shape for image models\n",
        "\n",
        "# Generate the visualization\n",
        "dot = make_dot(model(dummy_input), params=dict(model.named_parameters()))\n",
        "\n",
        "# Save the visualization to a PNG file\n",
        "dot.render(\"model\", format=\"png\")\n",
        "\n",
        "# Display it inline in a Jupyter/Colab notebook\n",
        "from IPython.display import Image\n",
        "display(Image(filename='model.png'))\n"
      ],
      "metadata": {
        "id": "EvGZizwmYOYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "g8JjSl2jFEIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, history = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)"
      ],
      "metadata": {
        "id": "h-Ov805TaEYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Model on the Test Set"
      ],
      "metadata": {
        "id": "irFttGXyFI0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate_model(model, test_loader, criterion)\n",
        "print(f\"Test Accuracy: {test_acc}%, Test Loss: {test_loss}\")"
      ],
      "metadata": {
        "id": "h5EEZ4mUaEWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Validation Metrics for Model Comparison"
      ],
      "metadata": {
        "id": "7OV4jrFtFMSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history['train_loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "#=============================================================\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "plt.title('Validation Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AL5gu2ISaEVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation and Performance Analysis"
      ],
      "metadata": {
        "id": "CWVm6Z_xV2tM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix, Classification Report and Predictions Section"
      ],
      "metadata": {
        "id": "eqxajc1eFTPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(model, data_loader, classes):\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []  # Predicted labels\n",
        "    all_labels = []  # True labels\n",
        "    all_probs = []  # Store probabilities for ROC-AUC calculation\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(outputs.softmax(dim=1).cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    return all_preds, all_labels, all_probs"
      ],
      "metadata": {
        "id": "Zir9-kdZaER4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds, all_labels, all_probs = plot_confusion_matrix(model, test_loader, class_names)"
      ],
      "metadata": {
        "id": "oFLOrjTCWc0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate classification metrics\n",
        "print('Classification Report:')\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names))"
      ],
      "metadata": {
        "id": "0VjIL0eJaEQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top-k Accuracy"
      ],
      "metadata": {
        "id": "NTRMuyrkSuSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Top-k accuracy measures the accuracy of the model when the true label is among the top-k predicted classes. This is especially useful for multi-class classification problems where the model might be correct but doesn't have the highest probability in the top-1 predicted class."
      ],
      "metadata": {
        "id": "UO5pE3tDS-UI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_accuracy(model, data_loader, k=5):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            # Ensure k does not exceed the number of classes\n",
        "            k = min(k, outputs.size(1))  # outputs.size(1) is the number of classes\n",
        "            _, top_k_preds = outputs.topk(k, 1, True, True)\n",
        "            correct += (top_k_preds == labels.view(-1, 1).expand_as(top_k_preds)).sum().item()\n",
        "            total += labels.size(0)\n",
        "    top_k_acc = correct / total\n",
        "    print(f'Top-{k} Accuracy: {top_k_acc * 100:.2f}%')\n",
        "    return top_k_acc\n",
        "\n",
        "top_k_accuracy(model, test_loader, k=5)"
      ],
      "metadata": {
        "id": "cU3f_CTuRuc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision, Recall, and F1-Score Calculation"
      ],
      "metadata": {
        "id": "-O66H8oFTRld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Precision, recall, and F1-score are essential metrics for understanding the model's performance in both imbalanced and balanced datasets. These metrics are included in the classification report but can be explicitly computed for deeper insight."
      ],
      "metadata": {
        "id": "Q__PclA4TKCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def precision_recall_f1(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n",
        "    return precision, recall, f1\n",
        "\n",
        "precision, recall, f1 = precision_recall_f1(model, test_loader)"
      ],
      "metadata": {
        "id": "OHjoQIJZTYSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Performance on a Validation Set"
      ],
      "metadata": {
        "id": "hFJZ-ZwDTdFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ When you have a separate validation dataset, it can be useful to evaluate the model performance on it separately to detect overfitting. This can be done by using evaluate_model() function and passing the validation dataset to it."
      ],
      "metadata": {
        "id": "0uqYYnMRTOjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation set\n",
        "val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
        "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n"
      ],
      "metadata": {
        "id": "FIj9XWoFTslj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class-wise Accuracy"
      ],
      "metadata": {
        "id": "WW5eyafpTwVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ For a more detailed analysis, checking how well a model performs on each individual class by calculating the accuracy for each class in the dataset."
      ],
      "metadata": {
        "id": "bC4v6FK_TPVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classwise_accuracy(model, data_loader, class_names):\n",
        "    model.eval()\n",
        "    correct = [0] * len(class_names)\n",
        "    total = [0] * len(class_names)\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for i in range(len(labels)):\n",
        "                total[labels[i]] += 1\n",
        "                if preds[i] == labels[i]:\n",
        "                    correct[labels[i]] += 1\n",
        "    classwise_acc = [correct[i] / total[i] * 100 if total[i] > 0 else 0 for i in range(len(class_names))]\n",
        "    for i, acc in enumerate(classwise_acc):\n",
        "        print(f'Accuracy for {class_names[i]}: {acc:.2f}%')\n",
        "    return classwise_acc\n",
        "\n",
        "classwise_accuracy(model, test_loader, class_names)"
      ],
      "metadata": {
        "id": "VtLAdsi5T_R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision-Recall Curve"
      ],
      "metadata": {
        "id": "56iwYHHkUKvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ When dealing with an imbalanced dataset, precision-recall curves can provide better insights than ROC curves."
      ],
      "metadata": {
        "id": "KdeMYM40Tz9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_precision_recall_curve(all_labels, all_probs, class_names):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        precision, recall, _ = precision_recall_curve(\n",
        "            (np.array(all_labels) == i).astype(int),  # Binarize the labels\n",
        "            np.array([prob[i] for prob in all_probs])\n",
        "        )\n",
        "        plt.plot(recall, precision, label=f'{class_name} (AUC = {auc(recall, precision):.2f})')\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.show()\n",
        "\n",
        "plot_precision_recall_curve(all_labels, all_probs, class_names)"
      ],
      "metadata": {
        "id": "d03y783lUS_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matthews Correlation Coefficient (MCC)"
      ],
      "metadata": {
        "id": "qXIl9AiFVC00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ MCC is a robust metric that takes into account true positives, true negatives, false positives, and false negatives."
      ],
      "metadata": {
        "id": "NLwKCnu7T0Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "def mcc_score(all_labels, all_preds):\n",
        "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "    print(f'Matthews Correlation Coefficient: {mcc:.4f}')\n",
        "    return mcc\n",
        "\n",
        "mcc_score(all_labels, all_preds)"
      ],
      "metadata": {
        "id": "rLYvFBj4VGXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Prediction Visualization"
      ],
      "metadata": {
        "id": "V6Gejx3pWrBR"
      }
    },
    {
      "source": [
        "# Map predicted labels to class names\n",
        "all_pred_class_names = [class_names[pred] for pred in all_preds]\n",
        "all_labels_class_names = [class_names[true] for true in all_labels]\n",
        "\n",
        "# Display sample predictions with color-coded labels\n",
        "print(\"Sample Predictions:\")\n",
        "num_samples = min(10, len(all_pred_class_names), len(all_labels_class_names))\n",
        "for i in range(num_samples):\n",
        "    pred_text = colored(all_pred_class_names[i], 'green') if all_pred_class_names[i] == all_labels_class_names[i] else colored(all_pred_class_names[i], 'red')\n",
        "    print(f\"Sample {i + 1}:\\n  True Label: {all_labels_class_names[i]}\\n  Predicted Label: {pred_text}\")\n",
        "    print(\"-\" * 30)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "a6YFmo4D2RbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Misclassified Sample Identification"
      ],
      "metadata": {
        "id": "Gd_YiK2KXFwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find and display misclassified samples\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "misclassified_samples = [\n",
        "    (i, true, pred) for i, (true, pred) in enumerate(zip(all_labels_class_names, all_pred_class_names)) if true != pred\n",
        "]\n",
        "\n",
        "if misclassified_samples:\n",
        "    print(f\"Total Misclassifications: {len(misclassified_samples)}\\n\")\n",
        "    for sample in misclassified_samples[:5]:\n",
        "        index, true_label, pred_label = sample\n",
        "        print(f\"Sample Index: {index}\")\n",
        "        print(f\"  True Label: {true_label}\")\n",
        "        print(f\"  Predicted Label: {colored(pred_label, 'red')}\")\n",
        "        print(\"-\" * 30)\n",
        "else:\n",
        "    print(\"No misclassifications detected!\")\n"
      ],
      "metadata": {
        "id": "CYlguADHXM2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROC-AUC Plotting for Multi-class Classifier"
      ],
      "metadata": {
        "id": "DPFeiqG7XPIo"
      }
    },
    {
      "source": [
        "# Binarize the labels for multi-class ROC-AUC calculation\n",
        "binarized_labels = label_binarize(all_labels, classes=list(range(len(class_names))))\n",
        "\n",
        "# Calculate ROC curve and AUC for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(len(class_names)):\n",
        "    fpr[i], tpr[i], _ = roc_curve(binarized_labels[:, i], [probs[i] for probs in all_probs])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot ROC curves for each class\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i, class_name in enumerate(class_names):\n",
        "    plt.plot(fpr[i], tpr[i], label=f'{class_name} (AUC = {roc_auc[i]:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YZ1PMhaP4Jlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Input"
      ],
      "metadata": {
        "id": "ynTDLl-iXcYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from IPython.display import display, HTML"
      ],
      "metadata": {
        "id": "bZI_t77pXoWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "External Image Prediction Utility"
      ],
      "metadata": {
        "id": "4reEjfOWYAON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    \"\"\"Preprocess an image for model prediction.\"\"\"\n",
        "    img_tf = tf.io.read_file(image_path)\n",
        "    img_tf = tf.image.decode_jpeg(img_tf, channels=3)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img_pil = Image.fromarray(img_tf.numpy())\n",
        "    return transform(img_pil)\n",
        "\n",
        "def predict_tumor_type(model, image_path):\n",
        "    \"\"\"Runs inference on a single image and returns the predicted tumor type.\"\"\"\n",
        "    img_torch = preprocess_image(image_path).unsqueeze(0).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(img_torch)\n",
        "        _, predicted_class = torch.max(output, 1)\n",
        "    return class_names[predicted_class.item()]\n"
      ],
      "metadata": {
        "id": "25bjdEVgXmWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tumor_type(model, image_path):\n",
        "    \"\"\"Predicts the tumor type using the trained model.\"\"\"\n",
        "    img_torch = preprocess_image(image_path).unsqueeze(0).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(img_torch)\n",
        "        _, predicted_class = torch.max(output, 1)\n",
        "    predicted_type = class_names[predicted_class.item()]\n",
        "    return predicted_type\n"
      ],
      "metadata": {
        "id": "efcrgpvPYG4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visual Display for Prediction"
      ],
      "metadata": {
        "id": "QEMdHco5YFDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_prediction(image_path, predicted_type):\n",
        "    \"\"\"Displays image with HTML-based prediction feedback.\"\"\"\n",
        "    img = Image.open(image_path)\n",
        "    display(img)\n",
        "    color = 'green' if predicted_type == \"healthy\" else 'red'\n",
        "    html_output = f\"\"\"\n",
        "    <div style=\"font-size: 18px; font-weight: bold; margin-top: 10px;\">\n",
        "        Prediction: <span style=\"color: {color};\">{predicted_type.capitalize()}</span>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html_output))\n"
      ],
      "metadata": {
        "id": "AhH32i4lYEjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting User Input for image path"
      ],
      "metadata": {
        "id": "xnnPBzBLYiHQ"
      }
    },
    {
      "source": [
        "# Get user input\n",
        "image_path = input(\"Enter the path to the image: \")\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(image_path):\n",
        "    predicted_type = predict_tumor_type(model, image_path)\n",
        "    display_prediction(image_path, predicted_type)\n",
        "else:\n",
        "    print(\"Error: Invalid image path.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CIFvB46jC6q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the Model"
      ],
      "metadata": {
        "id": "-LwEirBIFr-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "torch.save(model.state_dict(), 'model.pt')"
      ],
      "metadata": {
        "id": "byMlH698FtuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new ResNet-50 model\n",
        "loaded_resnet50 = models.resnet50(pretrained=False)\n",
        "\n",
        "# Load the saved weights\n",
        "loaded_resnet50.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "# Set to evaluation mode\n",
        "loaded_resnet50.eval()"
      ],
      "metadata": {
        "id": "8piv6psGFvSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model successfully loaded!\")\n",
        "print(loaded_resnet50)"
      ],
      "metadata": {
        "id": "Qsoku306FwFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a Simple CNN Model for Comparison"
      ],
      "metadata": {
        "id": "8_Z2UqhJF5ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
        "        self.fc2 = nn.Linear(128, 4)  # 4 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize CNN model\n",
        "cnn_model = SimpleCNN().to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the CNN model\n",
        "cnn_model, cnn_history = train_model(cnn_model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10)\n"
      ],
      "metadata": {
        "id": "lMGJY199F7Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate CNN on test set\n",
        "test_loss2, test_acc2 = evaluate_model(cnn_model, test_loader, criterion)\n",
        "print(f\"CNN Test Accuracy: {test_acc2:.2f}%\")\n"
      ],
      "metadata": {
        "id": "N6Akjb18mqeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "r18XoF3tnR51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ In this project, we successfully developed and evaluated a deep learning model for the classification of brain tumors using MRI images. Leveraging PyTorch and advanced image augmentation techniques from Albumentations, we created a robust and efficient classification pipeline. Our approach demonstrated promising accuracy and performance metrics, highlighting the potential of deep learning in assisting medical professionals with early and reliable tumor diagnosis.\n",
        "\n",
        "Through preprocessing, careful model design, and evaluation using metrics like ROC-AUC, we validated our model's capability in differentiating between tumor and non-tumor MRI images. The results affirm that deep learning can serve as a supportive tool for radiologists and neurologists in making faster and more accurate diagnoses."
      ],
      "metadata": {
        "id": "rfQESW2LTARc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing Model Performances"
      ],
      "metadata": {
        "id": "yOCOaq1UGNKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Pretrained models outperformed custom-built CNNs, highlighting the advantages of transfer learning in medical image classification tasks. Among them, ResNet18 proved to be both computationally efficient and accurate, making it the most suitable choice for this project."
      ],
      "metadata": {
        "id": "KOeDMXx4UKlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"ResNet-50 Test Accuracy: {test_acc:.2f}%\")\n",
        "print(f\"Custom CNN Test Accuracy: {test_acc2:.2f}%\")\n"
      ],
      "metadata": {
        "id": "qMRNOPY0GRCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÆ Future Work"
      ],
      "metadata": {
        "id": "qc11SJ9FVw_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explore Other Architectures and Hyperparameter Tuning:\n",
        "  *   Reasoning: Experimenting with different deep learning architectures like EfficientNet, MobileNet, or InceptionNet could potentially improve performance. Fine-tuning hyperparameters such as learning rate, batch size, and optimizer settings can further optimize the model.\n",
        "\n",
        "  *   Implementation: You can replace the ResNet50 model with other architectures using the torchvision.models library. Use techniques like grid search or Bayesian optimization for hyperparameter tuning.\n",
        "\n",
        "2. Incorporate 3D Information:\n",
        "  *   Reasoning: MRI scans are inherently 3D data. Utilizing 3D convolutional neural networks (CNNs) or recurrent neural networks (RNNs) could leverage the spatial information in the scans for better feature extraction and classification.\n",
        "\n",
        "  *   Implementation: Libraries like MONAI or MedicalTorch provide tools for working with 3D medical images. You would need to adapt your data loading and model architecture to handle 3D volumes.\n",
        "\n",
        "3. Dataset Expansion and Diversity:\n",
        "  *   Reasoning: Training on a larger and more diverse dataset can enhance model generalization and robustness. Include data from different sources and populations to reduce bias and improve performance on unseen cases.\n",
        "\n",
        "  *   Implementation: Explore publicly available datasets like the Brain Tumor Segmentation (BraTS) challenge dataset or collaborate with medical institutions to acquire more data.\n",
        "\n",
        "4. Segmentation and Localization:\n",
        "  *   Reasoning: Extend the project to not only classify tumor types but also segment the tumor regions within the MRI scans. This provides valuable information for treatment planning and monitoring.\n",
        "\n",
        "  *   Implementation: Use segmentation models like U-Net or Mask R-CNN to identify tumor boundaries. Employ metrics like Dice coefficient and IoU to evaluate segmentation performance.\n",
        "\n",
        "5. Deployment and Real-world Application:\n",
        "  *   Reasoning: Develop a user-friendly interface or integrate the model into a clinical workflow for real-world deployment. This could involve creating a web application or a mobile app for accessing predictions.\n",
        "\n",
        "  *   Implementation: Consider using frameworks like Flask or Django for web development. Explore cloud platforms like Google Cloud or AWS for deploying the model as a service.\n",
        "\n",
        "6. Explainability and Interpretability:\n",
        "  *   Reasoning: Gain insights into the model's decision-making process to build trust and understanding. Techniques like Grad-CAM or LIME can visualize the important features used by the model for classification.\n",
        "\n",
        "  *   Implementation: Apply explainability methods to understand which regions in the MRI scans are most influential for the model's predictions. This can aid in identifying potential biases or areas for improvement."
      ],
      "metadata": {
        "id": "a4CCEKycVzQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "mEClHiBrorBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   Hany Kasban, Mohsen El-bendary and Dina Salama, \"A Comparative Study of Medical Imaging Techniques\", International Journal of Information Science and Intelligent System, vol. 4, 2015. [Google Scholar](https://scholar.google.com/scholar?as_q=A+Comparative+Study+of+Medical+Imaging+Techniques&as_occt=title&hl=en&as_sdt=0%2C31)\n",
        "\n",
        "*   D. Surya Prabha and J. Satheesh Kumar, \"Performance Evaluation of Image Segmentation using Objective Methods\", Indian Journal of Science and Technology, vol. 9, no. 8, February 2016.\n",
        "[CrossRef](https://indjst.org/articles/performance-evaluation-of-image-segmentation-using-objective-methods)   [Google Scholar](https://scholar.google.com/scholar?as_q=Performance+Evaluation+of+Image+Segmentation+using+Objective+Methods&as_occt=title&hl=en&as_sdt=0%2C31)\n",
        "\n",
        "*   Brain Tumor: Statistics, 11 2017.\n",
        "[Google Scholar](https://scholar.google.com/scholar?as_q=Brain+Tumor%3A+Statistics&as_occt=title&hl=en&as_sdt=0%2C31)"
      ],
      "metadata": {
        "id": "P9g9EQcTI6UC"
      }
    }
  ]
}